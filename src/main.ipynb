{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJECT 2 \n",
    "(Natural Language Processing)\n",
    "### AUTHORS\n",
   "[UJJWAL KUMAR : 260730680]\n",
    "\n",
    "[ABDULLAHI ELMI : 260727124]\n",
    "\n",
    "[FURAHA DAMIEN : 260754407]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import os\n",
    "from sklearn import datasets\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classes = ['comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware',\n",
    "           'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos',\n",
    "           'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'talk.politics.misc',\n",
    "           'talk.politics.guns', 'talk.politics.mideast', 'sci.crypt', 'sci.electronics',\n",
    "           'sci.med', 'sci.space', 'talk.religion.misc', 'alt.atheism', 'soc.religion.christian']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOADING DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# twenty_test = None\n",
    "# twenty_train = None\n",
    "# docs_test = None\n",
    "# '''\n",
    "# NOT FOR .ipynb files\n",
    "#  Ask user for which data set they wanna test'''\n",
    "# dataset_name =sys.argv[1]\n",
    "# print(\"You choose dataset : \", sys.argv)\n",
    "# if(dataset_name == \"imdb\"):\n",
    "#     #  Training set for IMDB datasets\n",
    "#     twenty_train = datasets.load_files(\"../dataset/data/train\", description=None, categories=None,\n",
    "#                                        load_content=True, shuffle=True, encoding='utf-8', decode_error='strict', random_state=0)\n",
    "#     #  IMDB test dataset\n",
    "#     twenty_test = datasets.load_files(\"../dataset/data/test\", description=None, categories=None,\n",
    "#                                       load_content=True, shuffle=True, encoding='utf-8', decode_error='strict', random_state=0)\n",
    "#     docs_test = twenty_test.data\n",
    "# else:\n",
    "#     # Training set for 20 news groups dataset\n",
    "#     twenty_train = fetch_20newsgroups(\n",
    "#         subset='train', categories=classes, shuffle=True, random_state=42)\n",
    "#     # TEST SET for 20 new group\n",
    "#     twenty_test = fetch_20newsgroups(\n",
    "#         subset='test', categories=classes, shuffle=True, random_state=42)\n",
    "#     docs_test = twenty_test.data\n",
    "\n",
    "\n",
    "# load data\n",
    "path = os.getcwd()\n",
    "print(\".... Loading training DATASET..... \")\n",
    "print(\".....Done......\")\n",
    "''' Training set for IMDB datasets'''\n",
    "twenty_train = datasets.load_files(\"../dataset/data/train\", description=None, categories=None,\n",
    "                                   load_content=True, shuffle=True, encoding='utf-8', decode_error='strict', random_state=0)\n",
    "\n",
    "''' Training set for 20 news groups dataset '''\n",
    "# twenty_train = fetch_20newsgroups(\n",
    "#     subset='train', categories=classes, shuffle=True, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEXT PROCESSING PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PRE-PROCESSING\n",
    "    constructing bag of words\n",
    "    tokenizing and filtering of stopwords\n",
    "    feature extraction\n",
    "    counts of N-grams of words\n",
    "    downscale word weights by frequency\n",
    "\"\"\"\n",
    "# creating pipeline\n",
    "\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]\n",
    "\n",
    "\n",
    "print(\"Creating text processing pipline... \")\n",
    "\n",
    "def PIPELINE(classifier):\n",
    "    pipeline_1 = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', classifier)\n",
    "                           ])\n",
    "    pipeline_2 = Pipeline([('vect', CountVectorizer(tokenizer=LemmaTokenizer(),\n",
    "                                                    strip_accents='unicode',  # works\n",
    "                                                    stop_words='english',  # works\n",
    "                                                    lowercase=True,)), ('tfidf', TfidfTransformer()), ('clf', classifier)\n",
    "                           ])\n",
    "    return pipeline_1\n",
    "\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print(X_train_tfidf.shape)\n",
    "\n",
    "\n",
    "print(\".... Loading testing DATASET..... \")\n",
    "print(\".....Done......\")\n",
    "# TEST SET\n",
    "# twenty_test = fetch_20newsgroups(subset='test',\n",
    "#                                  categories=classes, shuffle=True, random_state=42)\n",
    "\n",
    "''' IMDB test dataset'''\n",
    "twenty_test = datasets.load_files(\"../dataset/data/test\", description=None, categories=None,\n",
    "                                  load_content=True, shuffle=True, encoding='utf-8', decode_error='strict', random_state=0)\n",
    "docs_test = twenty_test.data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN CLASSIFIERS\n",
    "# =====================LOGISTIC REGRESSION========================\n",
    "print(\"================== LOGISTIC REGRESSION =======================\")\n",
    "classifier = LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
    "                                intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
    "                                multi_class='auto', n_jobs=None, penalty='l2',\n",
    "                                random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
    "                                warm_start=False)\n",
    "# pipeline = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', classifier)\n",
    "#                      ])\n",
    "pipeline = PIPELINE(classifier)\n",
    "\n",
    "pipeline.fit(twenty_train.data, twenty_train.target)\n",
    "predicted = pipeline.predict(docs_test)\n",
    "\n",
    "''' Classification for 20 news group dataset'''\n",
    "# print(metrics.classification_report(twenty_test.target, predicted,\n",
    "#                                     target_names=twenty_test.target_names))\n",
    "'''Classification report for IMDB dataset '''\n",
    "print(metrics.classification_report(twenty_test.target, predicted))\n",
    "\n",
    "# print(metrics.confusion_matrix(twenty_test.target, predicted))\n",
    "print(\"Raw accurracy = %.3f%%\" %\n",
    "      ((np.mean(predicted == twenty_test.target))*100.0))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRID SEARCH\n",
    "(LOGISTIC REGRESSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression parameter tuning with GridSearchCV\n",
    "# TODO: Add more hyperparameter tuning\n",
    "# pipeline = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', LogisticRegression())\n",
    "#                      ])\n",
    "# regularization hyperparameter space\n",
    "\n",
    "hyperparameters = {\n",
    "    'clf__C': [0.1, 1, 100, 1000],\n",
    "    # 'clf__penalty': ['l1', 'l2'],\n",
    "    'clf__max_iter': [7600, 1000],\n",
    "    'clf__tol': [0.00001, 0.0001, 0.005]\n",
    "}\n",
    "grid_logistic = GridSearchCV(pipeline, hyperparameters, cv=5, )\n",
    "\n",
    "clf_best = grid_logistic.fit(twenty_train.data, twenty_train.target)\n",
    "print(\"<< REPORT >>\")\n",
    "print(\"Best score accurracy = %.3f%%\" % ((clf_best.best_score_)*100.0))\n",
    "print(\"Best parameters are:\")\n",
    "print(clf_best.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECISION TREES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== DECISION TREES ========================\n",
    "print(\"================== DECISION TREES =======================\")\n",
    "clf_decision_trees = DecisionTreeClassifier(random_state=0)\n",
    "# pipeline = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', clf_decision_trees)\n",
    "#  ])\n",
    "pipeline = PIPELINE(clf_decision_trees)\n",
    "pipeline.fit(twenty_train.data, twenty_train.target)\n",
    "predicted = pipeline.predict(docs_test)\n",
    "\n",
    "\n",
    "''' Classification for 20 news group dataset'''\n",
    "# print(metrics.classification_report(twenty_test.target, predicted,\n",
    "#                                     target_names=twenty_test.target_names))\n",
    "'''Classification report for IMDB dataset '''\n",
    "print(metrics.classification_report(twenty_test.target, predicted))\n",
    "# print(metrics.confusion_matrix(twenty_test.target, predicted))\n",
    "\n",
    "\n",
    "print(\"raw accurracy = %.3f%%\" %\n",
    "      ((np.mean(predicted == twenty_test.target))*100.0))\n",
    "print(cross_val_score(pipeline, twenty_train.data, twenty_train.target, cv=10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRID SEARCH\n",
    "(DECISION TREES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision trees parameter tuning with GridSearchCV\n",
    "# pipeline = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', DecisionTreeClassifier())\n",
    "#                      ])\n",
    "hyperparameters = {\n",
    "    'clf__criterion': ['gini', 'entropy'],\n",
    "    'clf__splitter': ['best', 'random'],\n",
    "    'clf__max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'clf__class_weight': ['balanced']\n",
    "}\n",
    "grid_d_trees = GridSearchCV(pipeline, hyperparameters, cv=5)\n",
    "clf_best = grid_d_trees.fit(\n",
    "    twenty_train.data, twenty_train.target)\n",
    "print(\"<< REPORT >>\")\n",
    "print(\"Best score accurracy = %.3f%%\" % ((clf_best.best_score_)*100.0))\n",
    "print(\"Best parameters are:\")\n",
    "print(clf_best.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUPPORT VECTOR MACHINE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================SVM - LINEARSVC ========================\n",
    "print(\"================== SUPPORT VECTOR MACHINE =======================\")\n",
    "classifier = LinearSVC(loss='hinge', penalty='l2',\n",
    "                       random_state=0, max_iter=76000, tol=1e-5)\n",
    "# clf = Pipeline([\n",
    "#     ('vect', CountVectorizer()),\n",
    "#     ('tfidf', TfidfTransformer()),\n",
    "#     ('clf', LinearSVC(loss='hinge', penalty='l2',\n",
    "#                       random_state=0, max_iter=76000, tol=1e-5)),\n",
    "# ])\n",
    "clf = PIPELINE(classifier)\n",
    "clf.fit(twenty_train.data, twenty_train.target)\n",
    "predicted = clf.predict(docs_test)\n",
    "\n",
    "\n",
    "''' Classification for 20 news group dataset'''\n",
    "# print(metrics.classification_report(twenty_test.target, predicted,\n",
    "#                                     target_names=twenty_test.target_names))\n",
    "'''Classfication report for IMDB dataset  '''\n",
    "print(metrics.classification_report(twenty_test.target, predicted))\n",
    "\n",
    "# print(metrics.confusion_matrix(twenty_test.target, predicted))\n",
    "\n",
    "\n",
    "print(\"raw accurracy = %.3f%%\" %\n",
    "      ((np.mean(predicted == twenty_test.target))*100.0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRID SEARCH\n",
    "(SUPPORT VECTOR MACHINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinearSVC parameter tuning with GridSearchCV\n",
    "# clf=Pipeline([\n",
    "#     ('vect', CountVectorizer()),\n",
    "#     ('tfidf', TfidfTransformer()),\n",
    "#     ('clf', LinearSVC())\n",
    "# ])\n",
    "\n",
    "parameters = {\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'clf__tol': [1e-2, 1e-4, 1e-9],\n",
    "    'clf__max_iter': [7600, 1000]\n",
    "}\n",
    "\n",
    "gs_clf = GridSearchCV(clf, parameters, cv=5, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "print(\"<< REPORT >>\")\n",
    "print(\"Best score accurracy = %.3f%%\" % ((gs_clf.best_score_)*100.0))\n",
    "print(\"Best parameters are : \")\n",
    "print(gs_clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADABOOST CLASSIFIER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================ADABOOST CLASSIFIER========================\n",
    "print(\"================== ADABOOST CLASSIFIER =======================\")\n",
    "classifier = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "# clf_adaboost = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', AdaBoostClassifier(n_estimators=100, random_state=0))\n",
    "#                          ])\n",
    "clf_adaboost = PIPELINE(classifier)\n",
    "clf_adaboost.fit(twenty_train.data, twenty_train.target)\n",
    "predicted = clf_adaboost.predict(docs_test)\n",
    "\n",
    "\n",
    "''' Classification for 20 news group dataset'''\n",
    "# print(metrics.classification_report(twenty_test.target, predicted,\n",
    "#                                     target_names=twenty_test.target_names))\n",
    "\n",
    "'''Classification for MDB dataset '''\n",
    "print(metrics.classification_report(twenty_test.target, predicted))\n",
    "\n",
    "# print(metrics.confusion_matrix(twenty_test.target, predicted))\n",
    "\n",
    "\n",
    "print(\"raw accurracy = %.3f%%\" %\n",
    "      ((np.mean(predicted == twenty_test.target))*100.0))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRID SEARCH\n",
    "(ADABOOST CLASSIFIER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoostClassifier parameter tuning with GridSearchCV\n",
    "# clf = Pipeline([\n",
    "#     ('vect', CountVectorizer()),\n",
    "#     ('tfidf', TfidfTransformer()),\n",
    "#     ('clf', AdaBoostClassifier())\n",
    "# ])\n",
    "\n",
    "parameters = {\n",
    "    'clf__n_estimators': [20, 50, 70, 100],\n",
    "    'clf__learning_rate': [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "}\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\n",
    "gs_clf = GridSearchCV(clf_adaboost, parameters, cv=kfold, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "print(\"<< REPORT >>\")\n",
    "print(\"Best score accurracy = %.3f%%\" % ((gs_clf.best_score_)*100.0))\n",
    "print(\"Best parameters are : \")\n",
    "print(gs_clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FOREST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================RANDOM FOREST========================\n",
    "print(\"================== RANDOM FOREST =======================\")\n",
    "\n",
    "classifier = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "# clf_random_forest = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', RandomForestClassifier(max_depth=2, random_state=0))\n",
    "# ])\n",
    "clf_random_forest = PIPELINE(classifier)\n",
    "clf_random_forest.fit(twenty_train.data, twenty_train.target)\n",
    "predicted = clf_random_forest.predict(docs_test)\n",
    "\n",
    "\n",
    "# ======= meteric classification\n",
    "''' Classification for 20 news group dataset'''\n",
    "# print(metrics.classification_report(twenty_test.target, predicted,\n",
    "#                                     target_names=twenty_test.target_names))\n",
    "\n",
    "'''Classification for IMDB dataset '''\n",
    "print(metrics.classification_report(twenty_test.target, predicted))\n",
    "# print(metrics.confusion_matrix(twenty_test.target, predicted))\n",
    "\n",
    "\n",
    "print(\"raw accurracy = %.3f%%\" %\n",
    "      ((np.mean(predicted == twenty_test.target))*100.0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRID SEARCH\n",
    "(RANDOM FOREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoostClassifier parameter tuning with GridSearchCV\n",
    "# clf_random_forest=Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', RandomForestClassifier())\n",
    "#                               ])\n",
    "param_grid = {\n",
    "    'clf__n_estimators': [200, 500],\n",
    "    'clf__max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'clf__max_depth': [4, 5, 6, 7, 8],\n",
    "    'clf__criterion': ['gini', 'entropy']\n",
    "}\n",
    "s\n",
    "grid_random_forest = GridSearchCV(\n",
    "    clf_random_forest, param_grid, cv=5, n_jobs=-1)\n",
    "grid_random_forest.fit(twenty_train.data, twenty_train.target)\n",
    "predicted = grid_random_forest.predict(docs_test)\n",
    "\n",
    "print(\"<< REPORT >>\")\n",
    "print(\"Best score accurracy = %.3f%%\" %\n",
    "      ((grid_random_forest.best_score_)*100.0))\n",
    "print(\"best parameters are: \")\n",
    "print(grid_random_forest.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
